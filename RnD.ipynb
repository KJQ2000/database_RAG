{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd557844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž Jewelry Inventory SQL Generator (type 'exit' to quit)\n",
      "\n",
      "User's question:\n",
      "\n",
      "what is the total stock weight I ahve?\n",
      "\n",
      "Generated SQL query:\n",
      "SELECT SUM(stk_weight) AS total_stock_weight FROM STOCK;\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from constant import *\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Initialize ChatOpenAI with GPT-4o-mini\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=512,\n",
    "    verbose=False,\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Load your dictionary file (table & column descriptions)\n",
    "def load_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# Prepare prompt templates\n",
    "system_template = \"\"\"\n",
    "You are a PostgreSQL expert for a jewelry store.\n",
    "Given the database schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "And the meaning of each table and column:\n",
    "\n",
    "{dictionary}\n",
    "\n",
    "Generate ONLY a valid PostgreSQL SELECT SQL query to answer the user question.\n",
    "Do not add explanations or markdown, just output the SQL.\n",
    "If you are not sure about the question, just output 'I am not sure on this question.'\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{user_question}\"\n",
    "\n",
    "def build_prompt(schema: str, dictionary: str, user_question: str):\n",
    "    system_msg = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    human_msg = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "    return prompt.format_prompt(schema=schema, dictionary=dictionary, user_question=user_question).to_messages()\n",
    "\n",
    "def generate_sql(user_question, schema, dictionary):\n",
    "    messages = build_prompt(schema, dictionary, user_question)\n",
    "    response = llm(messages)\n",
    "    return response.content\n",
    "\n",
    "schema_str = SCHEMA\n",
    "dictionary_str = load_file(DICT_DIR)\n",
    "\n",
    "print(\"ðŸ’Ž Jewelry Inventory SQL Generator (type 'exit' to quit)\")\n",
    "\n",
    "while True:\n",
    "    user_question = input(\"\\nAsk your question: \").strip()\n",
    "    if user_question.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    print(\"\\nUser's question:\\n\")\n",
    "    print(user_question)\n",
    "\n",
    "    sql = generate_sql(user_question, schema_str, dictionary_str)\n",
    "    print(\"\\nGenerated SQL query:\\n\" + sql)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfef1d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’Ž Jewelry Inventory Question Answering (type 'exit' to quit)\n",
      "\n",
      "User's question:\n",
      " è¯·åˆ—å‡ºæ‰€æœ‰å®¢æˆ·ï¼ˆCustomerï¼‰çš„å§“åå’Œè”ç³»æ–¹å¼ï¼Œä»¥åŠä»–ä»¬æœ€è¿‘ä¸€æ¬¡çš„é¢„è®¢ï¼ˆBookingï¼‰æ—¥æœŸå’Œè¯¥é¢„è®¢çš„æ€»ä»·ï¼ˆbook_priceï¼‰ã€‚å¦‚æžœå®¢æˆ·æ²¡æœ‰é¢„è®¢ï¼Œåˆ™æ˜¾ç¤ºé¢„è®¢ä¿¡æ¯ä¸ºç©ºã€‚\n",
      "\n",
      "Generated SQL query:\n",
      " SELECT c.cust_name, c.cust_phone_number, c.cust_email_address, b.book_date, b.book_price\n",
      "FROM konghin.customer c\n",
      "LEFT JOIN konghin.booking b ON c.cust_id = b.book_cust_id;\n",
      "\n",
      "Final Answer:\n",
      " æ ¹æ®æŸ¥è¯¢ç»“æžœï¼Œä»¥ä¸‹æ˜¯æ‰€æœ‰å®¢æˆ·çš„å§“åå’Œè”ç³»æ–¹å¼ï¼Œä»¥åŠä»–ä»¬æœ€è¿‘ä¸€æ¬¡çš„é¢„è®¢æ—¥æœŸå’Œè¯¥é¢„è®¢çš„æ€»ä»·ï¼š\n",
      "\n",
      "| å§“å         | è”ç³»ç”µè¯      | ç”µå­é‚®ä»¶                   | æœ€è¿‘é¢„è®¢æ—¥æœŸ  | æ€»ä»·     |\n",
      "|--------------|---------------|----------------------------|---------------|----------|\n",
      "| Kuan         | 012           | 213@gmail.com              | 2025-02-23    | 1000.00  |\n",
      "| test         | 554-1234      | john.doe@example.com       | 2024-12-12    | 0.00     |\n",
      "| test 2      | 555-8765      | alice.johnson@example.com  | 2024-12-12    | 80.00    |\n",
      "| test 3      | 555-4321      | bob.brown@example.com      | ç©º            | ç©º       |\n",
      "| test 1      | 555-5678      | jane.smith@example.com     | ç©º            | ç©º       |\n",
      "\n",
      "å¦‚æžœå®¢æˆ·æ²¡æœ‰é¢„è®¢ï¼Œåˆ™æ˜¾ç¤ºé¢„è®¢ä¿¡æ¯ä¸ºç©ºã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import logging\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from constant import *\n",
    "\n",
    "# ====== LOAD ENVIRONMENT ======\n",
    "load_dotenv()\n",
    "\n",
    "# ====== LOGGING SETUP ======\n",
    "log_file = \"db_query.log\"\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# ====== SIMPLE DATABASE CLASS ======\n",
    "class Database: \n",
    "    # def __init__(self, database_url: str):\n",
    "    def __init__(self, host: str,port: str,database: str,user: str,password: str):\n",
    "        # self.database_url = database_url\n",
    "        # self.conn = psycopg2.connect(self.database_url)\n",
    "        self.conn = psycopg2.connect(host=host,port=port,database=database,user=user,password=password)\n",
    "        # self.cursor = self.conn.cursor()\n",
    "        self.cursor = None\n",
    "        self.schema = 'konghin'\n",
    "        self.conn.autocommit = False\n",
    "\n",
    "        \n",
    "    def select_raw(self, query: str, params: tuple = None, js: bool = False):\n",
    "        \"\"\"\n",
    "        Executes a raw SQL query with optional parameters.\n",
    "\n",
    "        Args:\n",
    "            query (str): The SQL query to execute.\n",
    "            params (tuple, optional): Parameters to safely substitute into the query.\n",
    "            js (bool, optional): If True, returns the result as a JSON string.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame or str: DataFrame of results or JSON if js=True.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # self.refresh_connection()\n",
    "            \n",
    "            if self.cursor is None or self.cursor.closed:\n",
    "                self.cursor = self.conn.cursor()\n",
    "            \n",
    "            self.cursor.execute(query, params)\n",
    "            results = self.cursor.fetchall()\n",
    "            colnames = [desc[0] for desc in self.cursor.description]\n",
    "\n",
    "            df = pd.DataFrame(results, columns=colnames)\n",
    "\n",
    "            logging.info(f\"Successfully executed query: {query}\")\n",
    "\n",
    "            if js:\n",
    "                return df.to_json(orient='records', date_format='iso')\n",
    "            return df\n",
    "\n",
    "        except psycopg2.Error as e:\n",
    "            logging.error(f\"Database error: {e.pgcode} - {e.pgerror}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Query: {query.as_string(self.conn)}\")\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'cursor') and self.cursor:\n",
    "            self.cursor.close()\n",
    "        if hasattr(self, 'conn') and self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "# ====== LOAD DICTIONARY FILE ======\n",
    "def load_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# ====== LLM SETUP ======\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=512,\n",
    "    verbose=False,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# ====== PROMPT TEMPLATES ======\n",
    "system_template = \"\"\"\n",
    "You are a PostgreSQL expert for a jewelry store.\n",
    "Given the database schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "And the meaning of each table and column:\n",
    "\n",
    "{dictionary}\n",
    "\n",
    "Generate ONLY a valid PostgreSQL SELECT SQL query to answer the user question.\n",
    "pls add schema name before table name. Example: SELECT * FROM konghin.stock.\n",
    "Do NOT use double quotes (\"\") around schema, table names, or column names.\n",
    "Use lowercase for schema and table names.\n",
    "Do not add explanations or markdown, just output the SQL.\n",
    "If you are not sure about the question, just output 'I am not sure on this question.'\n",
    "\"\"\"\n",
    "human_template = \"{user_question}\"\n",
    "\n",
    "def build_prompt(schema: str, dictionary: str, user_question: str):\n",
    "    system_msg = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    human_msg = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
    "    return prompt.format_prompt(schema=schema, dictionary=dictionary, user_question=user_question).to_messages()\n",
    "\n",
    "def generate_sql(user_question, schema, dictionary):\n",
    "    messages = build_prompt(schema, dictionary, user_question)\n",
    "    response = llm(messages)\n",
    "    return response.content\n",
    "\n",
    "# ====== FUNCTION TO ANSWER QUESTIONS WITH DATA ======\n",
    "def answer_with_data(df: pd.DataFrame, user_question: str):\n",
    "    \"\"\"\n",
    "    Pass the DataFrame back to the LLM to answer the original question.\n",
    "    \"\"\"\n",
    "    data_str = df.to_csv(index=False)\n",
    "    analysis_prompt = f\"\"\"\n",
    "    You are a data analyst for a jewelry store. \n",
    "    The user asked: \"{user_question}\"\n",
    "    Here is the query result:\n",
    "    {data_str}\n",
    "\n",
    "    Based on this data, provide a concise and accurate answer.\n",
    "    If user ask in mandarin, pls reply in mandarin.\n",
    "    \"\"\"\n",
    "    messages = [HumanMessagePromptTemplate.from_template(analysis_prompt).format()]\n",
    "    response = llm(messages)\n",
    "    return response.content\n",
    "\n",
    "# ====== MAIN PROGRAM ======\n",
    "if __name__ == \"__main__\":\n",
    "    schema_str = SCHEMA\n",
    "    dictionary_str = load_file(DICT_DIR)\n",
    "\n",
    "    db = Database(\n",
    "        host=os.environ[\"HOST\"],\n",
    "        port=os.environ[\"PORT\"],\n",
    "        database=os.environ[\"DATABASE\"],\n",
    "        user=os.environ[\"USER\"],\n",
    "        password=os.environ[\"PASSWORD\"]\n",
    "        )\n",
    "\n",
    "    print(\"ðŸ’Ž Jewelry Inventory Question Answering (type 'exit' to quit)\")\n",
    "    while True:\n",
    "        user_question = input(\"\\nAsk your question: \").strip()\n",
    "        if user_question.lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "\n",
    "        print(\"\\nUser's question:\\n\", user_question)\n",
    "\n",
    "        # Step 1: Generate SQL\n",
    "        sql_query = generate_sql(user_question, schema_str, dictionary_str)\n",
    "        print(\"\\nGenerated SQL query:\\n\", sql_query)\n",
    "\n",
    "        if sql_query.strip().lower() == \"i am not sure on this question.\":\n",
    "            print(\"LLM could not generate a confident SQL query.\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Run SQL on Database\n",
    "        df = db.select_raw(sql_query)\n",
    "        if df is None or df.empty:\n",
    "            print(\"No results found or query error.\")\n",
    "            continue\n",
    "\n",
    "        # Step 3: Pass data back to LLM for final answer\n",
    "        final_answer = answer_with_data(df, user_question)\n",
    "        print(\"\\nFinal Answer:\\n\", final_answer)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbbfb8",
   "metadata": {},
   "source": [
    "## Intent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3939a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keong\\AppData\\Local\\Temp\\ipykernel_2021336\\3671568503.py:29: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  intent_llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "import os\n",
    "\n",
    "import yaml\n",
    "from src.shared.constant import *\n",
    "\n",
    "INTENT_SYSTEM_PROMPT = \"\"\"\n",
    "  You are an intent classification engine for a jewelry store chatbot.\n",
    "  \n",
    "  Classify the user's question into exactly ONE of the following intents:\n",
    "  \n",
    "  - general_knowledge\n",
    "  - database_query\n",
    "  - store_info\n",
    "  - unclear\n",
    "  \n",
    "  Rules:\n",
    "    - If question asks for numbers, counts, sales, stock, transactions â†’ database_query\n",
    "    - If question asks about return, exchange, warranty, policy â†’ store_policy\n",
    "    - If question asks about the store background, history, brand â†’ store_info\n",
    "    - If question is general world knowledge â†’ general_knowledge\n",
    "    - If unsure â†’ unclear\n",
    "  \n",
    "  Respond with ONLY the intent label.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "intent_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=10,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "def classify_intent(user_question: str) -> str:\n",
    "    messages = [\n",
    "        SystemMessage(content=INTENT_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=user_question)\n",
    "    ]\n",
    "    response = intent_llm(messages)\n",
    "    return response.content.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "207557eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general_knowledge\n",
      "store_info\n",
      "database_query\n",
      "unclear\n"
     ]
    }
   ],
   "source": [
    "gk_query = \"Who invented the light bulb?\"\n",
    "si_query = \"Where is your store located?\"\n",
    "db_query = \"Is item #12345 available for purchase?\"\n",
    "unclear_query = \"What about that thing?\"\n",
    "result1 = classify_intent(gk_query)\n",
    "print(result1)\n",
    "result2 = classify_intent(si_query)\n",
    "print(result2)\n",
    "result3 = classify_intent(db_query)\n",
    "print(result3)\n",
    "result4 = classify_intent(unclear_query)\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c3928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.shared.constant import *\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "import re\n",
    "\n",
    "def load_knowledge_base(file_path: str):\n",
    "    \"\"\"\n",
    "    Load a knowledge base file and normalize it into\n",
    "    texts + metadatas for vector database ingestion.\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "            {\n",
    "              \"texts\": List[str],\n",
    "              \"metadatas\": List[dict]\n",
    "            }\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Knowledge base file not found: {file_path}\")\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if ext == \".txt\":\n",
    "        return _load_txt_schema(file_path)\n",
    "\n",
    "    elif ext == \".csv\":\n",
    "        return _load_csv(file_path)\n",
    "\n",
    "    elif ext in [\".xls\", \".xlsx\"]:\n",
    "        return _load_excel(file_path)\n",
    "\n",
    "    elif ext == \".json\":\n",
    "        return _load_json(file_path)\n",
    "\n",
    "    elif ext in [\".yml\", \".yaml\"]:\n",
    "        return _load_yaml(file_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported file type '{ext}'. Supported types: CSV, Excel, JSON, YAML, TXT.\"\n",
    "        )\n",
    "\n",
    "def _load_txt_schema(file_path: str):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    # Split by separator lines\n",
    "    blocks = re.split(r\"=+\\n\", raw)\n",
    "\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for block in blocks:\n",
    "        block = block.strip()\n",
    "        if not block:\n",
    "            continue\n",
    "\n",
    "        # Extract English table name\n",
    "        table_match = re.search(r\"Table:\\s*(.+)\", block, re.IGNORECASE)\n",
    "        if not table_match:\n",
    "            continue\n",
    "\n",
    "        table_name = (\n",
    "            table_match.group(1)\n",
    "            .strip()\n",
    "            .lower()\n",
    "            .replace(\" \", \"_\")\n",
    "        )\n",
    "\n",
    "        texts.append(block)\n",
    "        metadatas.append({\n",
    "            \"type\": \"schema\",\n",
    "            \"table\": table_name,\n",
    "            \"source\": \"database\",\n",
    "            \"file\": os.path.basename(file_path)\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"texts\": texts,\n",
    "        \"metadatas\": metadatas\n",
    "    }\n",
    "\n",
    "def _load_csv(file_path: str):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        texts.append(row.to_string())\n",
    "        metadatas.append({\n",
    "            \"type\": \"table_row\",\n",
    "            \"row\": idx,\n",
    "            \"file\": os.path.basename(file_path)\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"texts\": texts,\n",
    "        \"metadatas\": metadatas\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_excel(file_path: str):\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        texts.append(row.to_string())\n",
    "        metadatas.append({\n",
    "            \"type\": \"table_row\",\n",
    "            \"row\": idx,\n",
    "            \"file\": os.path.basename(file_path)\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"texts\": texts,\n",
    "        \"metadatas\": metadatas\n",
    "    }\n",
    "\n",
    "def _load_json(file_path: str):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    text = json.dumps(data, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return {\n",
    "        \"texts\": [text],\n",
    "        \"metadatas\": [{\n",
    "            \"type\": \"json\",\n",
    "            \"file\": os.path.basename(file_path)\n",
    "        }]\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_yaml(file_path: str):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    text = yaml.dump(data, allow_unicode=True)\n",
    "\n",
    "    return {\n",
    "        \"texts\": [text],\n",
    "        \"metadatas\": [{\n",
    "            \"type\": \"yaml\",\n",
    "            \"file\": os.path.basename(file_path)\n",
    "        }]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef299226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "def get_openai_embedding_model():\n",
    "    \"\"\"\n",
    "    Centralized embedding factory.\n",
    "    Swappable in the future without refactoring.\n",
    "    \"\"\"\n",
    "    return OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        request_timeout=30,\n",
    "        max_retries=3\n",
    "    )\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "def build_documents(texts: list[str], metadatas: list[dict] | None = None):\n",
    "    \"\"\"\n",
    "    Converts raw text + metadata into LangChain Documents.\n",
    "    \"\"\"\n",
    "    if metadatas and len(texts) != len(metadatas):\n",
    "        raise ValueError(\"texts and metadatas must have same length\")\n",
    "\n",
    "    documents = []\n",
    "    for i, text in enumerate(texts):\n",
    "        documents.append(\n",
    "            Document(\n",
    "                page_content=text.strip(),\n",
    "                metadata=metadatas[i] if metadatas else {}\n",
    "            )\n",
    "        )\n",
    "    return documents\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def build_vector_database(\n",
    "    documents: list,\n",
    "    persist_path: str,\n",
    "    rebuild: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds and persists a FAISS vector database using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        documents: List[Document]\n",
    "        persist_path: Directory path to save vector DB\n",
    "        rebuild: If True, deletes existing DB and rebuilds\n",
    "\n",
    "    Returns:\n",
    "        FAISS vectorstore\n",
    "    \"\"\"\n",
    "\n",
    "    if rebuild and os.path.exists(persist_path):\n",
    "        shutil.rmtree(persist_path)\n",
    "\n",
    "    embedding_model = get_openai_embedding_model()\n",
    "\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model\n",
    "    )\n",
    "\n",
    "    os.makedirs(persist_path, exist_ok=True)\n",
    "    vectorstore.save_local(persist_path)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "def load_vector_database(persist_path: str):\n",
    "    \"\"\"\n",
    "    Loads a persisted FAISS vector database.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(persist_path):\n",
    "        raise FileNotFoundError(\"Vector database not found\")\n",
    "\n",
    "    embedding_model = get_openai_embedding_model()\n",
    "\n",
    "    return FAISS.load_local(\n",
    "        persist_path,\n",
    "        embedding_model,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "\n",
    "def retrieve_documents(\n",
    "    vectorstore,\n",
    "    query: str,\n",
    "    k: int = 4,\n",
    "    score: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents for a query.\n",
    "\n",
    "    Returns:\n",
    "        List[Document] or List[(Document, score)]\n",
    "    \"\"\"\n",
    "    if score:\n",
    "        return vectorstore.similarity_search_with_score(query, k=k)\n",
    "    return vectorstore.similarity_search(query, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13350a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = load_knowledge_base(KNOWLEDGE_BASE_PATH)\n",
    "\n",
    "\n",
    "documents = build_documents(\n",
    "    texts=kb[\"texts\"],\n",
    "    metadatas=kb[\"metadatas\"]\n",
    ")\n",
    "\n",
    "schema_vectorstore = build_vector_database(\n",
    "    documents=documents,\n",
    "    persist_path=\"vector_db/schema\",\n",
    "    rebuild=True\n",
    ")\n",
    "\n",
    "\n",
    "schema_vectorstore = load_vector_database(\"vector_db/schema\")\n",
    "\n",
    "docs = retrieve_documents(\n",
    "    schema_vectorstore,\n",
    "    query=\"How many gold rings sold last month?\",\n",
    "    k=10\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RnD_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
